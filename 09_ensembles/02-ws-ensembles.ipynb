{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvOZpUL1f23a"
   },
   "source": [
    "# Ансамбли моделей\n",
    "## Предисловие\n",
    "Как Вы уже знаете, модели МО можно использовать не только \"как есть\", но ещё и строить разнообразные надстройки над ними. Хорошей идеей такой надстройки было и остаётся объединение моделей в ансамбль.\n",
    "\n",
    "Сама суть ансамблей заключается в том, чтобы объединить предсказания стандартных моделей для повышения обощающей способности или надёжности ансамбля перед одиночным стандартным методом МО.\n",
    "\n",
    "Ансамбли делятся на два типа:\n",
    "\n",
    "- Голосующие (или усредняющие) - смысл их заключается в том, чтобы сначала обучить независимые друг от друга модели, а затем голосованием этих моделей выбирать предсказание (или усреднять предсказания по всем моделям, зависит от задачи). Обычно, качество предсказания у ансамблей лучше потому, что у них понижена дисперсия.\n",
    "\n",
    "    Примеры: [Бэггинг](https://scikit-learn.org/stable/modules/ensemble.html#bagging), [Случайный лес](https://scikit-learn.org/stable/modules/ensemble.html#forest)\n",
    "\n",
    "- Бустинг-ансамбли - последовательности стандартных моделей, построенные так, чтобы каждая последующая добавленная и натренированная модель каким-то образом исправляла ошибки всей предшествующей ей последовательности моделей. \"Объединение слабых моделей даёт сильную модель\".\n",
    "\n",
    "    Примеры: [AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost), [Градиентный бустинг](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)\n",
    "\n",
    "Полная [документация](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) по доступным в sklearn ансамблям.\n",
    "[Урок](https://habr.com/ru/company/ods/blog/327250/) про бустинг из курса хабра.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPOeDE6kuMRd"
   },
   "source": [
    "## Задача\n",
    "На этом воркшопе мы будем реализовывать алгоритм градиентного бустинга. Напомню, что в общем смысле бустинг заключается в том, чтобы тренировать каждую последующую модель так, чтобы она исправляла ошибки предыдущих.\n",
    "\n",
    "Прежде чем мы приступим к реализации, давайте вспомним, как он устроен:\n",
    "Краткое описание [оригинального алгоритма](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf) (который придумал Фридман):\n",
    "Для работы алгоритма помимо данных нужно подать:\n",
    "1. количество итераций $M \\in \\mathbb{N}$;\n",
    "2. функцию потерь $L(y,f)$, где $y$ - истинные значения, $f$ - полученная аппроксимация искомой функции (в данном случае это будет не функция, а просто предсказанные значения).\n",
    "Важно, чтобы функция потерь была дифференцируемой по $f$;\n",
    "3. базовая модель, на основе которой мы будем делать бустинг - в данном воркшопе мы воспользуемся деревьями решений;\n",
    "4. Начальное приближение $f_0(x)$ - чаще всего для этого используют некую константу.\n",
    "\n",
    "**Сам алгоритм:**\n",
    "1. Задаём начальное приближение функции $f(x) = f_0(x)$ константой - в данном случае можно выбрать ноль, среднее по столбцу с целевым признаком или что-нибудь ещё.\n",
    "2. Далее итеративно $t$ от 1 до $M$: \n",
    "    1. Считаем остатки как $\\large r_{t} = -\\left[\\frac{\\partial L(y, f(x))}{\\partial f(x)}\\right]_{f(x)=\\hat{f}(x)}$\n",
    "    2. Обучаем ещё одну базовую модель $h_t(x)$, при этом нецелевыми признаками у нас будет $x$, а в качестве целевого будем использовать остатки $r_t$, полученные на текущем шаге\n",
    "    3. Найти оптимальный коэффициент $\\rho_t = \\underset{\\rho}{\\arg\\min} \\ L(y, \\hat{f}(x) + \\rho \\cdot h(x))$ \n",
    "    4. обновить текущее приближение $\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\hat{f}_i(x)$, где $\\hat{f}_i(x) = \\rho_t \\cdot h_t(x)$\n",
    "3. Собрать все полученные базовые алгоритмы в модель $\\hat{f}(x) = \\sum_{i = 0}^M \\hat{f_i}(x)$\n",
    "\n",
    "**Функции потерь:**\n",
    "- $L(y, f) = (y - f)^2$ - L2-loss. Используйте его если считаете, что у модели нет никаких дополнительных требований к стабильности.\n",
    "- $L(y, f) = |y - f|$ - L1-loss. Применять при требованиях к стабильности модели. Из минусов можно выделить то, что её немного сложнее дифференцировать (придётся искать производные на разных кусках)\n",
    "- $\\begin{equation} L(y, f) =\\left\\{ \\begin{array}{@{}ll@{}} (1 - \\alpha) \\cdot |y - f|, & \\text{if}\\ y-f \\leq 0 \\\\ \\alpha \\cdot |y - f|, & \\text{if}\\ y-f >0 \\end{array}\\right. \\end{equation}, \\alpha \\in (0,1)$ - Lq-loss. Применять при наличии особых требований к модели, например, когда нам нужно восстановить не среднее и не медиану условного распределения $(y|x)$, а какую-нибудь квантиль. Штрафует наблюдения, попадающие выше $\\alpha$-той квантили.\n",
    "\n",
    "**Замечание:**\n",
    "1. коэффициенты $\\rho_i$ искать не обязательно, их можно считать равными единице\n",
    "2. функций потерь для регрессии бывает 3 вида, Вам достаточно реализовать хотя бы одну из них (конечно, будет здорово, если вы реализуете их все) и посчитать для неё градиент, тоже в виде функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFQvLM09s-h3"
   },
   "source": [
    "## Реализация алгоритма\n",
    "\n",
    "Как обычно, сначала импортируем всё, что нам нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_l5Vz59KWtL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAHLwjJis8S9"
   },
   "source": [
    "Теперь реализуйте функции потерь. Как я писал ранее, достаточно реализовать L2 loss или одну любую другую. Остальные - как дополнительное задание. Описания даны выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7o43E1QwGqW"
   },
   "outputs": [],
   "source": [
    "def l2_loss(y_real, y_predicted):\n",
    "    pass\n",
    "\n",
    "def l2_loss_gradient(y_real, y_predicted):\n",
    "    pass\n",
    "\n",
    "def l1_loss(y_real, y_predicted):\n",
    "    pass\n",
    "\n",
    "def l1_loss_gradient(y_real, y_predicted):\n",
    "    pass\n",
    "\n",
    "def lq_loss(y_real, y_predicted):\n",
    "    pass\n",
    "\n",
    "def lq_loss_gradient(y_real, y_predicted):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhMtSWwUtS86"
   },
   "source": [
    "Здесь мы просто упакуем функции в словарь, чтобы можно было передавать имя функции в алгоритм строкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mDM1iApGxc9"
   },
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'l2': (l2_loss, l2_loss_gradient),\n",
    "    'l1': (l1_loss, l1_loss_gradient),\n",
    "    'lq': (lq_loss, lq_loss_gradient),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2gn8mkitdkM"
   },
   "source": [
    "### Теперь приступим к реализации самого алгоритма. \n",
    "\n",
    "**Описание полей класса:**\n",
    "\n",
    "- `GradientTreeBoosting.estimators` - список, в котором мы будем хранить обученные деревья;\n",
    "- `GradientTreeBoosting.tree_kwargs` - параметры конструктора для деревьев, такие как глубина дерева, max features, и т.д. Подробнее в [документации](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor);\n",
    "- `GradientTreeBoosting.iteration` - $M$ из формул выше - количество итераций бустинга, и, одновременно, количество моделей, которые мы построим и объединим в ансамбль в результате бустинга;\n",
    "- `GradientTreeBoosting.loss` - функция потерь;\n",
    "- `GradientTreeBoosting.loss_grad` - градиент этой функции потерь.\n",
    "\n",
    "Методы `fit` и `predict` с таким же назначением, как мы и раньше писали.\n",
    "\n",
    "В методе `fit` Вам надо будет строить деревья, не забудьте передать им `tree_kwards` в качестве параметров конструктора. \n",
    "\n",
    "Обратите внимание, что по-умолчанию в качестве функции потерь указан l2-loss. Если вы его не реализовали, то измените на что-нибудь другое(или каждый раз указывайте явно или напишите try-except-блок)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j5kRdQzia0c"
   },
   "outputs": [],
   "source": [
    "class GradientTreeBoosting:\n",
    "    def __init__(self, loss='l2', iterations=10, **tree_kwargs):\n",
    "        self.estimators = []\n",
    "        self.tree_kwargs = tree_kwargs\n",
    "        self.iterations = iterations\n",
    "        self.loss, self.loss_grad = losses.get(loss)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        ### Ваш код ###\n",
    "\n",
    "    def predict(self, x):\n",
    "        ### Ваш код ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dYkkmqKtjah"
   },
   "source": [
    "### Проверка на игрушечном примере\n",
    "\n",
    "Сгенерируем выборку на основе косинуса и проверим, на сколько хорошо бустинг деревьев справится с этой задачей. Сравнивать будем с обычным деревом с такими же параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "Wk5tNuOGX6Xx",
    "outputId": "aef1f2eb-7d58-42b7-ee86-55a9ce50a857"
   },
   "outputs": [],
   "source": [
    "# генерируем данные\n",
    "np.random.seed(42)\n",
    "x = np.random.uniform(low=-2*np.math.pi, high=2*np.math.pi, size=(300))\n",
    "y = np.cos(x) + np.random.randn(300) * 0.2\n",
    "\n",
    "# обучаем обычное дерево глубины 3\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "tree.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "# обучаем наш бустинг-ансамбль\n",
    "clf = GradientTreeBoosting(iterations=3, max_depth=3, random_state=42)\n",
    "clf.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "# визуализация\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.scatterplot(x=x, y=y, label='original data')\n",
    "sns.scatterplot(x=x, y=clf.predict(x.reshape(-1, 1)), label='boosting')\n",
    "sns.scatterplot(x=x, y=tree.predict(x.reshape(-1, 1)), label='tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhhVDno3Peup"
   },
   "source": [
    "### Бустинг на практике: прогнозирование оттока пользователей из мобильной сети\n",
    "\n",
    "Будем решать задачу классификации (sic!), предсказывая, уйдёт ли пользователь мобильной сети или нет.\n",
    "Взято [отсюда](https://www.kaggle.com/mnassrib/telecom-churn-datasets).\n",
    "\n",
    "Данные и их типы: \n",
    "- State: string\n",
    "- Account length: integer \n",
    "- Area code: integer\n",
    "- International plan: string\n",
    "- Voice mail plan: string\n",
    "- Number vmail messages: integer\n",
    "- Total day minutes: double\n",
    "- Total day calls: integer\n",
    "- Total day charge: double\n",
    "- Total eve minutes: double\n",
    "- Total eve calls: integer\n",
    "- Total eve charge: double\n",
    "- Total night minutes: double\n",
    "- Total night calls: integer\n",
    "- Total night charge: double\n",
    "- Total intl minutes: double\n",
    "- Total intl calls: integer\n",
    "- Total intl charge: double\n",
    "- Customer service calls: integer\n",
    "- Churn: string - целевой признак - уйдёт или нет (на самом деле pandas понимает его как булев)\n",
    "\n",
    "Предсказывать будем логиты, то есть вероятность того, что клиент уйдёт, и на основе её уже делать прогноз, уйдёт ли он на самом деле или нет.\n",
    "\n",
    "Вам предстоит самостоятельно провести эксперимент. Сравните качество нашей бустинг модели с обычными методами МО (вспомните и подумайте, какие лучше).\n",
    "\n",
    "В качестве метрики качества используйте `f1_score` (а ещё лучше, `classification_report`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzxc8HxaOFFJ"
   },
   "source": [
    "#### Загрузите данные и предобработайте их.\n",
    "\n",
    "Разбейте данные на тренировочную и тестовую подвыборки.\n",
    "\n",
    "Поскольку деревья из sklearn не поддерживают категориальные признаки Вам придётся решить эту проблему - удалить их или обработать известными нам способами (one-hot-encoding, счётчики и тп).\n",
    "\n",
    "В описании к датасету предполагалось использовать больший датасет(churn-bigml-80.csv) для обучения, а меньший(churn-bigml-20.csv) для проверки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LO6LafYuDPBY"
   },
   "outputs": [],
   "source": [
    "### Ваш код ###\n",
    "# создайте больше ячеек, если нужно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBktZHUJBRgp"
   },
   "source": [
    "Здесь я немного помогу Вам. Функция `test_qual` позволяет проверить качество классификации без лишних телодвижений. Воспользуйтесь ей (если хотите, можете модифицировать как Вам удобно). В исходном виде она не подойдёт для финальной проверки (на разных датасетах), но вы можете сделать промежуточные выводы на её основе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UB9ICK9v5UYB"
   },
   "outputs": [],
   "source": [
    "def decide_class(logits, threshold=0.5):\n",
    "    res = np.zeros(logits.shape[0])\n",
    "    res[logits >= threshold] = 1\n",
    "    return res\n",
    "\n",
    "def test_qual(model, X, y, test_size=0.2, threshold=0.5):\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    test_logits = model.predict(X_test)\n",
    "    y_pred = decide_class(test_logits, threshold=threshold)\n",
    "\n",
    "    # print(accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ensemble_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
